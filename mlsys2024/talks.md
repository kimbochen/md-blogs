# Kurt
#MLSys2024 Invited talk by Kurt Keutzer
ML -> NN -> Transformers -> LLM -> Compound Gen AI Sys


The right prospecive contex or point of view is worth 80 IQ points alan kay

mkaing decisions under uncertainty based on observed data


2010s - 2020s: We thought model architecture was the key to improvement


early ml and dl: single modality & analytical
next-gen LLM: synthetic & generative

more improvements from training data disribution search (data splits + oversampling factor ratios
- karpathy

memory wall: gap between compute and memory speed, llm excacerbating it

models are prone to hallucination: the way forward is agents and co-pilots

if youre really interested in efficiency, understand the aplications

tiny agent + llm comipler: managing day-to-day tasks


# Azalia

data - model - software - hardware

data: constitutional ai

model: mixture of experts

**a new frontier for scaling is inference**

throughput performance: tokens / s >> FLOP / s


Gemini is MoE?

models improve with sampling


scaling inference with hardware-aware optimization


sparsity in activations mimics a mixture of experts mechanism

Future directions:
scaling methods: AI for AI systems (Code optimization, CUDA kernel design)
systems that enable scaling: decentralized MoE; co-design model/SW/HW

**Question: scaling laws vs. emergent properties?**


# Xupeng


# Jiawei

GaLore future works: support distributed training / low-interconnect-bandwidth clusters
